{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "Haiku has a built in MultiHeadAttention layer, but it does not include causal masking. The multi-head attention block can be extended to build a masked self-attention block. The block accepts a query, key, value as well as the mask and returns the output as a JAX array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(hk.MultiHeadAttention):\n",
    "    def __call__(self, query: jnp.ndarray, key: Optional[jnp.ndarray] = None, value: Optional[jnp.ndarray] = None, mask: Optional[jnp.ndarray] = None) -> jnp.ndarray:\n",
    "        key = key if key is not None else query\n",
    "        value = value if value is not None else query\n",
    "\n",
    "        seq_len = query.shape[1]\n",
    "        casual_mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "        mask = mask * casual_mask if mask is not None else casual_mask\n",
    "\n",
    "        return super().__call__(query, key, value, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(hk.Module):\n",
    "    \"\"\"A 2-layer MLP\"\"\"\n",
    "\n",
    "    def __init__(self, init_scale: float, widening_factor: int = 4, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self._init_scale = init_scale\n",
    "        self._widening_factor = widening_factor\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        hiddens = x.shape[-1]\n",
    "        initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
    "        x = hk.Linear(self._widening_factor * hiddens, w_init=initializer)(x)\n",
    "        x = jax.nn.gelu(x)\n",
    "\n",
    "        return hk.Linear(hiddens, w_init=initializer)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x: jnp.ndarray, name: Optional[str] = None) -> jnp.ndarray:\n",
    "    \"\"\"Apply a unique layer normalization to x with default settings\"\"\"\n",
    "    return hk.LayerNorm(axis=-1, create_scale=True, create_offset=True, name=name)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(hk.Module):\n",
    "    \"\"\"Transformer Stack\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, num_layers: int, dropout_rate: float, name: Optional[str] = None):\n",
    "        super().__init__(name)\n",
    "        self._num_heads = num_heads\n",
    "        self._num_layers = num_layers\n",
    "        self._dropout_rate = dropout_rate\n",
    "\n",
    "\n",
    "    def __call__(self, h: jnp.ndarray, mask: Optional[jnp.ndarray] = None, is_training: bool = True) -> jnp.ndarray:\n",
    "        \"\"\"\n",
    "        Connects the transformer.\n",
    "\n",
    "        Args:\n",
    "            h: Inputs, [B, T, H].\n",
    "            mask: Padding mask, [B, T] // the mask to apply to the attention scores\n",
    "            is_training: Whether we are in training mode\n",
    "\n",
    "        Returns:\n",
    "            Array of shape [B, T, H].\n",
    "        \"\"\"\n",
    "\n",
    "        init_scale = 2. / self._num_layers\n",
    "        dropout_rate = self._dropout_rate if is_training else 0.\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :]\n",
    "\n",
    "        for i in range(self._num_layers):\n",
    "            h_norm = layer_norm(h, name=f\"h{i}_ln_1\")\n",
    "            h_attn = SelfAttention(num_heads=self._num_heads, key_size=64, w_init_scale=init_scale, name=f\"h{i}_attn\")(h_norm, mask=mask)\n",
    "            h_attn = hk.dropout(hk.next_rng_key(), dropout_rate, h_attn)\n",
    "            h = h + h_attn\n",
    "            h_norm = layer_norm(h, name=f'h{i}_ln_2')\n",
    "            h_dense = DenseBlock(init_scale, name=f'h{i}_mlp')(h_norm)\n",
    "            h_dense = hk.dropout(hk.next_rng_key(), dropout_rate, h_dense)\n",
    "            h = h + h_dense\n",
    "        h = layer_norm(h, name='h_ln_f')\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(data: Mapping[str, jnp.ndarray], vocab_size: int) :\n",
    "    tokens = data['obs']\n",
    "    input_mask = jnp.greater(tokens, 0)\n",
    "    seq_length = tokens.shape[1]\n",
    "\n",
    "    # Embed the input tokens and positions.\n",
    "    embed_init = hk.initializers.TruncatedNormal(stddev=0.02)\n",
    "    token_embedding_map = hk.Embed(vocab_size, d_model, w_init=embed_init)\n",
    "    token_embs = token_embedding_map(tokens)\n",
    "    positional_embeddings = hk.get_parameter(\n",
    "        'pos_embs', [seq_length, d_model], init=embed_init)\n",
    "    input_embeddings = token_embs + positional_embeddings\n",
    "    return input_embeddings, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_forward_fn(vocab_size: int, d_model: int, num_heads: int,\n",
    "                     num_layers: int, dropout_rate: float):\n",
    "    \"\"\"Create the model's forward pass.\"\"\"\n",
    "\n",
    "    def forward_fn(data: Mapping[str, jnp.ndarray],\n",
    "                   is_training: bool = True) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        input_embeddings, input_mask = embeddings(data, vocab_size)\n",
    "\n",
    "        # Run the transformer over the inputs.\n",
    "        transformer = Transformer(\n",
    "            num_heads=num_heads, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "        output_embeddings = transformer(input_embeddings, input_mask, is_training)\n",
    "\n",
    "        # Reverse the embeddings (untied).\n",
    "        return hk.Linear(vocab_size)(output_embeddings)\n",
    "\n",
    "    return forward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss_fn(forward_fn,\n",
    "               vocab_size: int,\n",
    "               params,\n",
    "               rng,\n",
    "               data: Mapping[str, jnp.ndarray],\n",
    "               is_training: bool = True) -> jnp.ndarray:\n",
    "    \"\"\"Compute the loss on data wrt params.\"\"\"\n",
    "    logits = forward_fn(params, rng, data, is_training)\n",
    "    targets = jax.nn.one_hot(data['target'], vocab_size)\n",
    "    assert logits.shape == targets.shape\n",
    "\n",
    "    mask = jnp.greater(data['obs'], 0)\n",
    "    loss = -jnp.sum(targets * jax.nn.log_softmax(logits), axis=-1)\n",
    "    loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
