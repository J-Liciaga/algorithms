{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning Tiny LLMs as Expert Agents\n",
    "\n",
    "Tiny LLMs have never been ideal for agentic workflows. They lack the ability to reliably generate function calls; however, this isn't due to any real limitation on LLM size. Instead, it's due to the LLM providers' lack of focus on training data that provides quality examples of function calling\n",
    "\n",
    "Because of that, we can fine-tune expert agents from tiny LLMs such as the 1B parameter Llama 3.2 and get incredible results. In this example, we do just that - we take llama-3.2-1b-instruct, Salesforce's xLAM dataset, and Low-Rank Adaptation (LoRA) fine-tuning via NVIDIA's NeMo Microservices, to create our own tiny LLM agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU \\\n",
    "    datasets==3.6.0 \\\n",
    "    graphai-lib==0.0.5 \\\n",
    "    openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "To train our LLM for function-calling we need a dataset containing function calls. Salesforce released exactly that with the Salesforce/xlam-function-calling-60k dataset. This dataset was used by Salesforce to train their family of Large Action Models (LAMs). These LAMs were designed specifically for function calling, reasoning, and planning — all essential abilities for agents.\n",
    "\n",
    "We can download the dataset from HuggingFace, to do so we do need an account as we must agree to the T&Cs to use this dataset. After you have agreed to the T&Cs we first authenticate ourselves by grabbing a read-only token from the hub and entering it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "token = getpass(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    \"Salesforce/xlam-function-calling-60k\",\n",
    "    split=\"train\",\n",
    "    token=token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of this dataset contains a user query, a set of tools that an LLM has access to, and the correct tool call that should be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is not the format we need for training on NeMo — instead we need the standardized OpenAI format containing a list of messages (with roles of user or assistant) and a tools JSON containing a list of function schemas which defines the tools available to our LLM. It looks like this:\n",
    "```json\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"<user query>\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\", \"tool_calls\": [\n",
    "            {\n",
    "                \"id\": \"call_xyz\", \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"<tool name>\", \"arguments\": {<input args>}\n",
    "                }\n",
    "            },\n",
    "            ... <other calls if running parallel tool calling>\n",
    "        ]}\n",
    "    ],\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"<tool name>\",\n",
    "                \"description\": \"<natural language description of the tool>\",\n",
    "                \"parameteres\": {  # this defines all possible args and their types\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"type\": {\n",
    "                            \"type\": \"<data type, like `string`>\",\n",
    "                            \"description\": \"<field-specific description>\",\n",
    "                            \"default\": \"<optional field, use for default values>\"\n",
    "                        },\n",
    "                        \"required\": []  # list required params, ie params without 'default'\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "Our defined tools is simple a list of function schemas, ie functions that we would typically define that an LLM will be able to call. For example, we could define a multiply function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: float, b: float, round: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Multiplies two numbers together. Rounding is optional.\n",
    "    \"\"\"\n",
    "    return (a * b) if round else a * b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xLAM formatted function schemas are not aligned with the OpenAI format. Note the lack of 'type': 'function', different parameters structure, and different types (xLAM uses Python types like str or List[Any], which would become string or array respectively when using the OpenAI format).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(data[0][\"tools\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the xLAM data format into the OpenAI format we will work through a few transformation steps, first we handle type transformation from Python format tot OpenAI format, summarized here:\n",
    "\n",
    "| Python format | OpenAI format | Explanation |\n",
    "|---------------|---------------|-------------|\n",
    "| \"str\" | \"string\" | Maps primitive Python type to OpenAI type |\n",
    "| \"int\" | \"integer\" | Same as above |\n",
    "| \"float\" |\t\"number\" | |\t\n",
    "| \"bool\" | \"boolean\" | |\t\n",
    "| \"list\" or \"List\" | \"array\" | Maps list-like types to array |\n",
    "| \"dict\" or \"Dict\" | \"object\" | Maps dict-like types to object |\n",
    "| \"Set\" or \"set\" | \"array\" | Sets are arrays in JSON schema |\n",
    "| \"Callable[[int], str]\" | \"string\" | Callables treated as strings |\n",
    "| \"Tuple[int, str]\" | \"array\" | Tuples treated as arrays |\n",
    "| \"List[str], default='foo'\" | \"array\" | Strips default, then normalizes |\n",
    "| \"default='foo'\" | \"string\" | If only default value, assume string |\n",
    "| \"str, optional\" | \"string\" | Removes , optional |\n",
    "| \"UnknownType\" | \"string\" | Defaults unknown types to string |\n",
    "\n",
    "We apply these transformations with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_type(param_type: str) -> str:\n",
    "    param_type = param_type.strip().split(\",\")[0]\n",
    "\n",
    "    if param_type.startswith(\"default=\"):\n",
    "        return \"string\"\n",
    "\n",
    "    param_type = param_type.replace(\", optional\", \"\").strip()\n",
    "\n",
    "    if any(param_type.startswith(prefix) for prefix in (\"Callable\", \"Tuple\", \"List[\", \"Set\")):\n",
    "        return \"array\" if \"Tuple\" in param_type or \"List\" in param_type or \"Set\" in param_type else \"string\"\n",
    "\n",
    "    type_mapping = {\n",
    "        \"str\": \"string\",\n",
    "        \"int\": \"integer\",\n",
    "        \"float\": \"number\",\n",
    "        \"bool\": \"boolean\",\n",
    "        \"list\": \"array\",\n",
    "        \"dict\": \"object\",\n",
    "        \"List\": \"array\",\n",
    "        \"Dict\": \"object\",\n",
    "        \"set\": \"array\",\n",
    "        \"Set\": \"array\",\n",
    "    }\n",
    "\n",
    "    return type_mapping.get(param_type, \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we restructure the tool/function schemas from xLAM to OpenAI format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import json\n",
    "\n",
    "def xlam_tools_to_openai(\n",
    "    tools: str | list[dict[str, Any]]\n",
    ") -> list[dict[str, Any]]:\n",
    "    # if input is string we assume it is json so parse it\n",
    "    if isinstance(tools, str):\n",
    "        try:\n",
    "            tools = json.loads(tools)\n",
    "        except json.JSONDecodeError:\n",
    "            # if error, return empty list\n",
    "            return []\n",
    "\n",
    "    # check we have a list, if not return empty list\n",
    "    if not isinstance(tools, list):\n",
    "        return []\n",
    "\n",
    "    openai_tools = []\n",
    "    \n",
    "    for tool in tools:\n",
    "        # check tool is dictionary with parameters dict inside\n",
    "        if not isinstance(tool, dict) or not isinstance(tool.get(\"parameters\"), dict):\n",
    "            # if not, we don't want it\n",
    "            continue\n",
    "\n",
    "        properties = {}\n",
    "\n",
    "        for name, info in tool[\"parameters\"].items():\n",
    "            # skip if param info isn't a dict\n",
    "            if not isinstance(info, dict):\n",
    "                continue\n",
    "\n",
    "            # convert from python -> openai types\n",
    "            param = {\n",
    "                \"description\": info.get(\"description\", \"\"),  # default to empty string\n",
    "                \"type\": normalize_type(info.get(\"type\", \"\")),\n",
    "            }\n",
    "\n",
    "            # include default if it's not None or empty string\n",
    "            default = info.get(\"default\")\n",
    "            if default not in (None, \"\"):\n",
    "                param[\"default\"] = default\n",
    "\n",
    "            properties[name] = param\n",
    "\n",
    "        # build new function format\n",
    "        openai_tools.append({\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.get(\"name\", \"\"),\n",
    "                \"description\": tool.get(\"description\", \"\"),\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": properties\n",
    "                },\n",
    "            },\n",
    "        })\n",
    "\n",
    "    return openai_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, our xLAM tool schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(data[0][\"tools\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is transformed into this OpenAI tool schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlam_tools_to_openai(json.loads(data[0][\"tools\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That will handle our transformation for the function schemas, but we also need to reformat our dataset into the correct OpenAI messages format. We do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlam_tool_calls_to_openai(tool_calls: list[dict]) -> list[dict]:\n",
    "    \"\"\"Convert xLAM tool calls to OpenAI tool calls.\"\"\"\n",
    "    # not all models support parallel tool calling, so we\n",
    "    # just look at records with a single tool call\n",
    "    if len(tool_calls) == 1:\n",
    "        return [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": tool_calls[0]\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the original xLAM format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(data[1][\"answers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the OpenAI version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlam_tool_calls_to_openai(json.loads(data[1][\"answers\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do this for both the user and assistant message in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlam_messages_to_openai(data: dict) -> dict:\n",
    "    \"\"\"Convert xLAM data format to OpenAI format.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": data[\"query\"]},\n",
    "        {\n",
    "            \"role\": \"assistant\", \"content\": \"\",\n",
    "            \"tool_calls\": xlam_tool_calls_to_openai(json.loads(data[\"answers\"]))\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This turns the xLAM query and answers into a messages list containing a user message followed by the assistant message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlam_messages_to_openai(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"messages\": xlam_messages_to_openai(data[1]),\n",
    "    \"tools\": xlam_tools_to_openai(json.loads(data[1][\"tools\"]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "openai_data = []\n",
    "\n",
    "for row in tqdm(data):\n",
    "    messages = xlam_messages_to_openai(row)\n",
    "    tools = xlam_tools_to_openai(json.loads(row[\"tools\"]))\n",
    "    if messages is None or messages[1][\"tool_calls\"] is None or tools is None:\n",
    "        # invalid data so we skip\n",
    "        continue\n",
    "    else:\n",
    "        openai_data.append({\n",
    "            \"messages\": messages,\n",
    "            \"tools\": tools\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Vaidation-Test Split\n",
    "\n",
    "We will split our dataset into a train-validation-test split, with 70% for training, 15% for validation, and 15% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(openai_data)\n",
    "\n",
    "train_split_index = int(len(openai_data) * 0.7)\n",
    "val_split_index = int(len(openai_data) * 0.85)\n",
    "# create split datasets\n",
    "train_data = openai_data[:train_split_index]\n",
    "val_data = openai_data[train_split_index:val_split_index]\n",
    "test_data = openai_data[val_split_index:]\n",
    "\n",
    "print(f\"Train data: {len(train_data)}\")\n",
    "print(f\"Val data: {len(val_data)}\")\n",
    "print(f\"Test data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would typically be using the test_data in evaluation, which requires a slightly different format to the OpenAI format we have created already, for this format we must shift the tool_calls data from the assistant message into a separate tool_calls key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"messages\": [x[\"messages\"][0]],\n",
    "        \"tools\": x[\"tools\"],\n",
    "        \"tool_calls\": x[\"messages\"][1][\"tool_calls\"]\n",
    "    } for x in test_data\n",
    "]\n",
    "\n",
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training data\n",
    "with open(\"training.jsonl\", \"w\") as fp:\n",
    "    for row in train_data:\n",
    "        fp.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "# save validation data\n",
    "with open(\"validation.jsonl\", \"w\") as fp:\n",
    "    for row in val_data:\n",
    "        fp.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "# save test data\n",
    "with open(\"test.jsonl\", \"w\") as fp:\n",
    "    for row in test_data:\n",
    "        fp.write(json.dumps(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Models Prep\n",
    "\n",
    "Before we can train a model with the NeMo Customizer service, we need to push our data to the NeMo Data Store and push our base model to the NeMo Entity Store. Before doing either of those things we need to ensure our NeMo microservices are deployed and running. To do that, we will list all services running within our demo namespace that have a name matching the wildcard nemo-*, we do this with kubectl like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMESPACE = \"demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get service -n {NAMESPACE} | grep '^nemo-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to grab a few of the ClusterIP and hosts from above. These values do change so make sure you update with your own deployment endpoints in the format \"http://<cluster-ip>:<host>\". For example, if your nemo-data-store fields are:\n",
    "\n",
    "| NAME | TYPE |\tCLUSTER-IP | EXTERNAL-IP | PORT(S) | AGE |\n",
    "|------|------|------------|-------------|---------|-----|\n",
    "| nemo-data-store |\tClusterIP |\t10.111.16.88 | 3000/TCP | 2m2s |\n",
    "\n",
    "\n",
    "You will need to enter:\n",
    "\n",
    "```python\n",
    "DATA_STORE = \"http://10.111.16.88:3000\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMIZER = \"http://10.107.42.136:8000\"            # service/nemo-customizer\n",
    "DATA_STORE = \"http://10.102.137.118:3000\"           # service/nemo-data-store\n",
    "DEPLOYMENT_MANAGER = \"http://10.102.234.211:8000\"   # service/deployment-management\n",
    "ENTITY_STORE = \"http://10.111.17.85:8000\"           # service/nemo-entity-store\n",
    "NIM_URL = \"http://10.102.70.221:8000\"               # service/nemo-nim-proxy - 8000 for HTTP and 8001 for gRPC\n",
    "\n",
    "DATASET_NAME = \"xlam-ft-dataset\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find API docs for all NeMo Microservice APIs here.\n",
    "\n",
    "We first create a namespace where all of the resources and artifacts created during the tutorial will live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# create namespace in entity store\n",
    "res1 = requests.post(f\"{ENTITY_STORE}/v1/namespaces\", json={\"id\": NAMESPACE})\n",
    "# create namespace in data store\n",
    "res2 = requests.post(\n",
    "    f\"{DATA_STORE}/v1/datastore/namespaces\",\n",
    "    data={\"namespace\": NAMESPACE}\n",
    ")\n",
    "\n",
    "res1, res2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get our data uploaded to our microservices. We first create our data store repo. We use the HfApi client to do this but it's worth noting that we're not using Hugging Face Hub at all here. We're instead just piggy backing off their SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_id = f\"{NAMESPACE}/{DATASET_NAME}\"\n",
    "\n",
    "hf_api = HfApi(endpoint=f\"{DATA_STORE}/v1/hf\", token=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.errors import HfHubHTTPError\n",
    "\n",
    "try:\n",
    "    # check if the repo exists, if not, create it\n",
    "    hf_api.repo_exists(repo_id=repo_id, repo_type=\"dataset\")\n",
    "except HfHubHTTPError:\n",
    "    # this means the repo doesn't exist, so we create it\n",
    "    print(f\"Creating `{repo_id}` dataset\")\n",
    "    hf_api.create_repo(repo_id=repo_id, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ If needed, you can delete dataset repos with hf_api.delete_repo(repo_id=repo_id, repo_type=\"dataset\").\n",
    "\n",
    "Next we upload our training, validation, and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_api.upload_file(\n",
    "    path_or_fileobj=\"training.jsonl\",\n",
    "    path_in_repo=\"training.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=\"validation.jsonl\",\n",
    "    path_in_repo=\"validation.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=\"test.jsonl\",\n",
    "    path_in_repo=\"test.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we register the dataset with NeMo Entity Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.post(\n",
    "    url=f\"{ENTITY_STORE}/v1/datasets\",\n",
    "    json={\n",
    "        \"name\": DATASET_NAME,\n",
    "        \"namespace\": NAMESPACE,\n",
    "        \"description\": \"Tool calling xLAM dataset\",\n",
    "        \"project\": NAMESPACE,\n",
    "        \"files_url\": f\"/datasets/{NAMESPACE}/{DATASET_NAME}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ If you need to delete records in the entity store you can use DELETE {ENTITY_STORE}/v1/datasets/{NAMESPACE}/{DATASET_NAME}.\n",
    "\n",
    "Let's double check the dataset exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(f\"{ENTITY_STORE}/v1/datasets/{NAMESPACE}/{DATASET_NAME}\")\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to kick-off training. First, we'll choose a model that we'd like to train, we can see a list of available models by hitting the GET {CUSTOMIZER}/v1/customization/configs endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(f\"{CUSTOMIZER}/v1/customization/configs\")\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see any models that were defined in values.yaml inside customizer.customizerConfig.models. In this example, we should see meta/llama-3.2-1b-instruct.\n",
    "\n",
    "Now we hit POST /v1/customization/jobs to start training, before doing so, there are a few parameters that we should be aware of — most of which are covered in detail in the customization docs.\n",
    "\n",
    "Sequence Packing can be turned on or off, it is an optimization technique but it is only compatible with some Meta Llama models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta/llama-3.2-1b-instruct\"\n",
    "\n",
    "# add weights and biases API key for updates during training\n",
    "WANDB_API_KEY = getpass(\"Enter your W&B API key: \")\n",
    "headers = {\"wandb-api-key\": WANDB_API_KEY} if WANDB_API_KEY else None\n",
    "\n",
    "training_params = {\n",
    "    \"name\": \"llama-3.2-1b-xlam-ft\",\n",
    "    \"output_model\": f\"{NAMESPACE}/llama-3.2-1b-xlam-run1\",\n",
    "    \"config\": BASE_MODEL,\n",
    "    \"dataset\": {\"name\": DATASET_NAME, \"namespace\": NAMESPACE},\n",
    "    \"hyperparameters\": {\n",
    "        \"training_type\": \"sft\",\n",
    "        \"finetuning_type\": \"lora\",\n",
    "        \"epochs\": 2,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"lora\": {\n",
    "            \"adapter_dim\": 32,\n",
    "            \"adapter_dropout\": 0.1,\n",
    "        },\n",
    "        \"sequence_packing_enabled\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "res = requests.post(\n",
    "    f\"{CUSTOMIZER}/v1/customization/jobs\",\n",
    "    json=training_params,\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "customization = res.json()\n",
    "\n",
    "customization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⏳ If you see a message like {'detail': 'Model <model-name> is downloading to cache, try again later'} the custom model is still being downloaded to the model cache. Assuming everything is running correctly all you need to do in this scenario is wait. You can check for issues in the deployment or simply current download status like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pod -n {NAMESPACE} | grep '^model-downloader-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ If you see a message like {'detail': '<model-name> is not configured for training'} you need to configure available models via the values.yaml file created in our deployment.\n",
    "\n",
    "If our customization job is running we should see a large response detailing the training parameters and most importantly our customization job ID. We can use this ID to check in on the job status like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = customization[\"id\"]\n",
    "\n",
    "res = requests.get(f\"{CUSTOMIZER}/v1/customization/jobs/{job_id}/status\")\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the job is running in our cluster too — we should first see an entity-handler pod which should complete quickly and we will see a training-job pod appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pod -n {NAMESPACE} | awk 'NR==1 || /^cust-/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check logs with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl logs cust-xyz-training-job-worker-0 -n {NAMESPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But most useful, if you set your W&B API key earlier you can find the training data in your W&B dashboard. We can continue checking the job status until it completes (for the 1b parameter model on a H100 this can take ~50 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(f\"{CUSTOMIZER}/v1/customization/jobs/{job_id}/status\")\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running our New Model\n",
    "\n",
    "Once the training job is complete the custom model we have build should be available to us via the NeMo entity store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.post(\n",
    "    f\"{DEPLOYMENT_MANAGER}/v1/deployment/model-deployments\",\n",
    "    json={\n",
    "        \"name\": \"llama-3.2-1b\",\n",
    "        \"namespace\": \"meta\",\n",
    "        \"config\": {\n",
    "            \"model\": \"meta/llama-3.2-1b-instruct\",\n",
    "            \"nim_deployment\": {\n",
    "                \"image_name\": \"nvcr.io/nim/meta/llama-3.2-1b-instruct\",  # NGC catalog image URL\n",
    "                \"image_tag\": \"1.8.5\",\n",
    "                \"pvc_size\": \"25Gi\",\n",
    "                \"gpu\": 1,\n",
    "                \"additional_envs\": {\n",
    "                    \"NIM_GUIDED_DECODING_BACKEND\": \"fast_outlines\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check for the model-deployment job to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get all -n {NAMESPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the base model has been registered our NIM endpoint will detect it and automatically load all compatible models based on what we have inside our NeMo Entity Store, we can confirm that has happened with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(f\"{NIM_URL}/v1/models\")\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With training complete and our model usable by our NIM endpoint, we can jump into testing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our Model\n",
    "\n",
    "First, we setup our NIM client using the OpenAI client but swapping the base_url from OpenAI to our NIM proxy server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "nim = OpenAI(\n",
    "    base_url=f\"{NIM_URL}/v1\",\n",
    "    api_key=\"None\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use our NIM endpoint as we would the typical chat completions endpoint of OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = nim.chat.completions.create(\n",
    "    model=\"demo/llama-3.2-1b-xlam-run1@cust-xyz\",\n",
    "    messages=test_data[0][\"messages\"],\n",
    "    tools=test_data[0][\"tools\"],\n",
    "    tool_choice=\"auto\",\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    max_tokens=512,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "out.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stream like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = nim.chat.completions.create(\n",
    "    model=\"demo/llama-3.2-1b-xlam-run1@cust-xyz\",\n",
    "    messages=test_data[0][\"messages\"],\n",
    "    tools=test_data[0][\"tools\"],\n",
    "    tool_choice=\"auto\",\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    "    max_tokens=512,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
